## 残差网络

### 认知：

##### 1. Identity mapping

理论上来说，只要我们将前一层的输出原封不动给到后一层，深度值再大的情况下我们的结果都不应当差于小深度结果。

##### 2.残差

即，在接收原输入Z的网络层学习到X之后，我们不在深层的网络中学习H(X)，而是学习H(X)-X，输出F(X)+X

![image-20230718192250243](C:\Users\23850\AppData\Roaming\Typora\typora-user-images\image-20230718192250243.png)

PS：此处注意，使用的是relu

##### 3.实现特别点

- 瓶颈结构：为了减少参数数量和计算量，ResNet引入了瓶颈结构（Bottleneck Structure）。瓶颈结构由一个1x1卷积层、一个3x3卷积层和一个1x1卷积层组成，分别用于降低输入特征图的通道数、增加特征图的深度和恢复特征图的通道数。这样的设计可以在保持网络性能的同时减少计算量。
- 残差块：残差网络主要由残差块（Residual Block）构成。每个残差块包含了两个主要的部分：恒等映射（Identity Mapping）和残差映射（Residual Mapping）。恒等映射将输入直接传递到输出，而残差映射则通过堆叠多个层来学习输入与输出之间的残差。通过将恒等映射和残差映射相加，得到最终的输出。
- 残差网络的层间连接：除了残差块内的跳跃连接外，残差网络还引入了层间连接（Layer-wise Connection）。在训练过程中，残差网络通过将前一层的输出与后一层的输入相加，将信息直接传递给后面的层。这种连接方式有助于信息的流动和梯度的传播，减轻了梯度消失和梯度爆炸问题。

##### 4.在cv领域使用transformer的trick

不使用最原始的图片输入作为transformer的token，而是采用特征图，从而极大地降低了输入维度。